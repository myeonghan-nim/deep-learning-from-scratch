# chapter07. CNN

- **합성곱 신경망(CNN)**은 주로 이미지 인식 분야에 유용하게 쓰이는 딥러닝 기법입니다.

## 7.1 전체 구조

- CNN은 **합성곱 계층**과 **풀링 계층**을 조합해 만듭니다만 기존의 방식과 약간 차이가 있습니다.

- 우선 기존의 신경망은 입전하는 계층의 모든 뉴런과 결합된 **완전연결** 형태를 가졌습니다.

  - 이 계층을 **Affine 계층**이라고 불렀습니다.

<img src="README.assets/fig 7-1-1578400851611.png" alt="fig 7-1" style="zoom:50%;" />

> 즉, 완전연결 신경망은 Affine-ReLU 조합의 결과를 Softmax를 통해 출력합니다.

- 반면 CNN은 합성곱 계층과 풀링 계층이 새로 추가된 형태를 띄고 있습니다.

<img src="README.assets/fig 7-2.png" alt="fig 7-2" style="zoom:50%;" />

> 즉, CNN은 Conv(합성곱)-ReLU-(Pooling)(풀링)의 흐름으로 연결됩니다.

- 또한 CNN은 출력에 가까운 층에서 지금까지 사용한 Affine-ReLU 구성을 사용할 수 있고 출력 계층도 Affine-Softmax를 그대로 사용할 수 있습니다.

## 7.2 합성곱 계층

- CNN에서는 **패딩**, **스트라이드**와 같은 CNN고유의 용어가 등장합니다.

- 또한, 각 계측 사이에서 3차원 데이터 같은 입체적인 데이터가 흐른다는 점에서 완전연결 신경망과 다릅니다.

### 7.2.1 완전연결 계층의 문제점

- 지금까지 사용한 환전연결 신경망은 완전연결 계층(Affine 계층)을 사용했습니다.

  - 완전연결 계층은 인접하는 계층의 모든 뉴런이 연결되고 출력 수를 임의로 조절할 수 있습니다.

- 다만, 이 형태는 데이터의 형상이 무시된다는 단점을 지니고 있습니다.

  - 예를 들어 이미지의 경우 가로, 세로, 채널의 3차원 데이터지만 완전연결 계층에 입력할 때 1차원 데이터로 평탄화를 해야합니다.

  - 이는 이미지가 가지고 있는 공간적 정보를 무시하며 본질적인 패턴이 있을 수 있는 경우를 무시합니다.

- 반면에 합성곱 계층은 형상을 유지합니다. 따라서 입력된 데이터의 형상과 출력된 데이터의 형상을 유지합니다.

- CNN에서는 합성곱 계층의 입출력 데이터를 **특징 맵**이라고 합니다.(입력은 입력 특징 맵, 출력은 출력 특징 맵)

### 7.2.2 합성곱 연산

- 합성곱 계층에서 처리하는 **합성곱 연산**은 이미지 처리에서 말하는 **필터 연산**입니다.

<img src="README.assets/fig 7-3.png" alt="fig 7-3" style="zoom:50%;" />

- 위와 같이 합성곱 연산은 입력 데이터에 **필터**(혹은 **커널**)를 적용합니다.

  - 합성곱 연산은 필터의 **윈도우**를 일정 간격으로 이동해가며 입력 데이터에 적용합니다.

  - 이 이동의 결과에 따라 입력과 필터에 대응하는 원소의 곱의 합을 출력하는데 이를 **단일 곱셈 누산**이라고 합니다.

  - 이 과정을 나열하면 다음과 같습니다.

<img src="README.assets/fig 7-4.png" alt="fig 7-4" style="zoom:50%;" />

> 완전연결 신경망의 가중치와 편항 중 CNN의 필터의 매개변수가 가중치에 해당합니다.
>
> 여기에 편향이 적용되면 다음과 같습니다.

<img src="README.assets/fig 7-5.png" alt="fig 7-5" style="zoom:50%;" />

### 7.2.3 패딩

- **패딩**은 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값(예를 들어 0)으로 채우는 작업을 말합니다.(다음 그림을 참고하세요.)

<img src="README.assets/fig 7-6.png" alt="fig 7-6" style="zoom:50%;" />

- 이렇게 하면 입력 데이터와 동일한 형상의 출력데이터를 얻을 수 있습니다.

- 패딩의 크기는 원하는 대로 설정할 수 있으며(위 그림같은 경우는 1입니다.) 출력 데이터의 크기를 조절합니다.

#### Note

- 패딩을 주로 출력 크기를 조정할 목적으로 사용됩니다.

- 특히 합성곱 연산을 반복하는 심층 신경망에서 데이터의 크기가 작아지는 것을 방지하는 차원에서 주로 사용합니다.

### 7.2.4 스트라이드

- 필터를 적용하는 위치 간격을 **스트라이드**라고 합니다.

- 앞선 예시는 스트라이드가 1이었지만 다음과 같은 예시는 스트라이드가 2입니다.

<img src="README.assets/fig 7-7.png" alt="fig 7-7" style="zoom:50%;" />

- 위와 같이 연산을 지속하다보면 패팅을 크게하면 출력의 크기는 커지고 스트라이드를 키우면 출력의 크기는 작아집니다.

  - 이들의 상관 관계는 다음의 수식과 같습니다.

  <img src="README.assets/e 7.1.png" alt="e 7.1" style="zoom:50%;" />

  > H, W: 입력의 크기(세로, 가로), FH, FW: 필터의 크기, OH, OW: 출력의 크기, P: 패딩, S, 스트라이드

  - 다만, 출력의 크기가 정수로 나누어 떨어지게 끔 올바른 파라미터 값을 조절해야 합니다.

### 7.2.5 3차원 데이터의 합성곱 연산

- 다음과 같이 3차원 데이터는 3차원 방향으로 특징 맵이 늘어났습니다.

  - 그 결과 입력 데이터와 합성곱 연산을 채널마다 수행하고 그 결과를 더해 하나의 출력을 만듭니다.

<img src="README.assets/fig 7-8.png" alt="fig 7-8" style="zoom:50%;" />

> 조금 더 상세한 연산 과정은 다음과 같습니다.

<img src="README.assets/fig 7-9.png" alt="fig 7-9" style="zoom:50%;" />

- 이 때 입력 데이터의 채널 수와 필터의 채널 수가 같아야 함에 유의하세요.

### 7.2.6 블록으로 생각하기

- 3차원의 합성곱 연산은 데이터와 필터를 직육면체 블록으로 생각하면 더 쉽습니다.

> 즉, C, H, W 형태의 입력 데이터와 C, FH, FW 형태의 필터가 만나 1, OH, OW 형태의 출력 데이터를 얻습니다.

- 만약 연산의 출력으로 다수의 채널을 보내고 싶다면 필터를 다수 이용하는 다음과 같은 과정을 거치면 됩니다.

<img src="README.assets/fig 7-10.png" alt="fig 7-10" style="zoom:50%;" />

> 위와 같이 필터를 FN개 적용하면 출력도 FN개가 됩니다.

- 즉, 합성곱 연산은 필터의 수도 고려해야 합니다.

  - 그래서 필터의 가중치 데이터는 4차원 데이터(FN, C, FH, FW: 출력 채널 수, 입력 채널 수, 높이, 너비)입니다.

- 여기에 편향을 더하면 다음과 같습니다.

<img src="README.assets/fig 7-12.png" alt="fig 7-12" style="zoom:50%;" />

### 7.2.7 배치 처리

- 신경망에서 입력 데이터를 한 덩어리로 묶어 배치로 처리한 것 처럼 합성곱 연산도 배치 처리를 지원합니다.

  - 구체적으로는 각 계층을 흐르는 데이터의 차원을 하나 늘려 4차원 데이터(데이터 수, 채널 수, 높이, 너비)로 합니다.

<img src="README.assets/fig 7-13.png" alt="fig 7-13" style="zoom:50%;" />

> 즉, 위와 같이 4차원 데이터가 하나 흐르면 데이터 N개에 대한 합성곱 연산을 수행합니다.

## 7.3 풀링 계층

- 풀링이란 가로, 세로 방향의 공간을 줄이는 연산을 말합니다.

<img src="README.assets/fig 7-14.png" alt="fig 7-14" style="zoom:50%;" />

- 위는 2 by 2 **최대 풀링**을 스트라이드 2로 처리하는 순서 예시입니다.

  - 최대 풀링은 최댓값을 구하는 연산으로 2 by 2의 크기에서 가장 큰 원소를 꺼냅니다.

  - 여기에 스트라이드가 2이므로 윈도우가 2칸 간격으로 이동하게 됩니다.

  > 일반적으로 풀링의 크기와 스트라이드의 크기는 일치합니다.

> 이 외에도 **평균 풀링**(대상 영역의 평균을 계산) 등 다양한 풀링이 있습니다.

### 7.3.1 풀링 계층의 특징

- 풀링 계층의 특징은 다음과 같습니다.

  1. 학습해야 할 매개변수가 없습니다.

     - 풀링 게층은 합성곱 계층과 달리 학습해야 할 매개변수가 없습니다.

  2. 채널 수가 변하지 않습니다.

     - 풀링 연산은 입력 데이터의 채널 수 그대로 출력 데이터로 내보내는데 이는 채널마다 독립적으로 계산하기 때문입니다.

     <img src="README.assets/fig 7-15.png" alt="fig 7-15" style="zoom:50%;" />

  3. 입력의 변화에 영향을 적게 받습니다.

     - 입력 데이터가 조금 변하더라도 풀링의 결과는 잘 변하지 않습니다.

     <img src="README.assets/fig 7-16.png" alt="fig 7-16" style="zoom:50%;" />
