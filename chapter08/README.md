# chapter08. Deep Learning

- 딥러닝은 층을 깊게 한 심층 신경망입니다.

- 이 싱층 신경망은 지금까지 배운 신경망을 바탕으로 뒷단에 층을 추가하디만 하면 만들 수 있습니다.

- 하지만 그렇게 만들기엔 여러 문제점이 많습니다.

- 이번 장에서는 딥러닝의 특징과 과제, 가능성을 살펴보고 오늘날 첨단 딥러닝도 배울 계획입니다.

## 8.1 더 깊게

- 신경망에 관해 그동안 많은 것을 배웠습니다.

  - 신경망을 구성하는 다양한 계층과 학습에 효과적인 기술

  - 영상 분야에 특히 유요한 CNN과 매개변수 최적화 기법 등 다양한 것을 배웠습니다.

- 이 모두는 딥러닝에서 매우 중요한 기술입니다.

- 이번 절은 배운 기술을 집약하고 심층 신경망을 만들어 MNIST 데이터셋 손글씨 숫자 인식에 도전합니다.

### 8.1.1 더 깊은 신경망으로

- 이번 절에서 만들 신경망은 다음 그림과 같습니다.

<img src="README.assets/fig 8-1-1579613756450.png" alt="fig 8-1" style="zoom:50%;" />

- 이전보다 더 깊은 신경망으로 특성은 다음과 같습니다.

  - 합성곱 계층은 모두 (3, 3) 크기의 작은 필터로 층이 깊어지면서 채널 수가 늘어납니다.

    - 앞에서부터 순서대로 16, 16, 32, 32, 64, 64로 증가합니다.

  - 풀링 계층을 중간중간 추가하여 중간 데이터의 공간 크기를 점차 줄여나갑니다.

  - 마지막 단의 완전연결 계층에서 드롭아웃 계층을 사용합니다.

  - 가중치 초깃값은 He 초깃값(활성화 함수는 ReLU)이며 가중치 매개변수 갱신은 Adam을 사용합니다.

> 이 신경망의 정확도는 약 99.38%입니다.

#### Note

- 이 신경망을 구현한 코드는 8.1.1_deep_convnet.py에 기록되어 있습니다.

- 반면 훈련용 코드는 8.1.1_train_deepnet.py에 기록되어 있습니다.

- 이 코드들을 직접 실행시키는 것도 좋지만 반나절 이상 걸릴 것이므로 deep_conv_net_params.pkl에 학습된 매개변수가 준비되어 있습니다.

> 다음 그림은 신경망이 인식하지 못한 이미지들 입니다.

<img src="README.assets/fig 8-2.png" alt="fig 8-2" style="zoom:50%;" />

- 이 사진들은 인간도 판단하기 어려운 이미지입니다.

- 이런 결과에 비추어 볼 때 심층 CNN은 인간과 비슷하게 인식할 정도로 잠재력이 크다는 점을 알 수 있습니다.

### 8.1.2 정확도를 높이려면?

- 다음 사진은 What is the class of this image? 라는 웹 사이트에서 다양한 데이터셋을 대상으로 그동안 발표된 기법들의 정확도 순위입니다.

<img src="README.assets/fig 8-3.png" alt="fig 8-3" style="zoom:50%;" />

- 순위를 보면 Neural Networks나 Deep, Convolutional이라는 키워드가 자주 등장함을 알 수 있습니다.

  - 상위권의 기법들은 대부분 CNN을 기초로 한 것이지만 깊이가 그다지 깊지 않습니다.

#### Note

- MNIST 데이터셋은 층이 매우 깊지 않아도 최고 수준의 결과가 나옵니다.

- 반면에 나중에 소개할 대규모 일반 사물 인식은 문제가 훨씬 복잡해지므로 층을 깊게해 정확도를 올릴 필요가 있습니다.

> 위 그림의 상위 기법들을 참고하면 정확도를 더 높일 수 있는 기술이나 힌트를 발견할 수 있습니다.
>
> 예를 들어 앙상블 학습, 학습률 감소, 데이터 확장 등이 여기서 나왔습니다.

- 여기서 **데이터 확장**이란 입력 이미지를 알고리즘을 동원해 인위적으로 확장하는 것을 말합니다.

<img src="README.assets/fig 8-4.png" alt="fig 8-4" style="zoom:50%;" />

- 데이터 확장은 다양한 방법으로 이루어집니다.

  - 예를 들어 이미지 일부를 자르는 crop, 좌우를 뒤집는 flip 등이 있습니다.

  - 그 외에도 밝기 변화, 확대 및 축소 등 스케일 변화 등 다양한 방법이 있습니다.

### 8.1.3 깊게 하는 이유

- 층을 깊게 하는 것이 왜 중요한지는 다음과 같이 설명할 수 있습니다.

  0. ILSVRC와 같은 대규모 이미지 인식 대회의 결과를 볼 때 층이 깊어질수록 더 좋은 결과를 가져옵니다.

  1. 신경망의 매개변수 수가 줄어듭니다. 층이 깊어질수록 더 적은 매개변수로 더 나은 결과를 얻을 수 있습니다.

     > 다음 그림은 (5, 5) 필터로 구성된 합성곱 계층의 예시입니다.

     <img src="README.assets/fig 8-5.png" alt="fig 8-5" style="zoom:50%;" />

     > 다음 그림은 (3, 3) 필터를 두 번 거친 합성곱 계층의 예시입니다.

     <img src="README.assets/fig 8-6.png" alt="fig 8-6" style="zoom:50%;" />

     - 여기서 (3, 3) 필터의 계산 결과는 (5, 5) 필터가 계산한 영역에서 계산한 결과와 비슷합니다.

     - 즉, (5, 5) 연산 1회는 (3, 3) 연산 2회로 대체할 수 있으면 매개변수도 25개에서 18개로 줄어듭니다.

     - 이 차이는 층이 깊어질수록 커지게 됩니다.

#### Note

- 작은 필터를 겹쳐 신경망을 깊게 할 때 장점은 매개변수 수를 줄여 넓은 **수용 영역**을 소화할 수 있는 점입니다.

  > 수용 영역이란 뉴런에 변화를 일으키는 국소적인 공간 영역을 의미합니다.

- 또한 층을 거듭하면 활성화 함수를 합성곱 계층 사이에 끼움으로써 신경망의 표현력이 개선됩니다.

  - 이는 활성화 함수가 신경망에 비선형 힘을 가하고 비선형 함수가 겹치면서 복잡한 것도 표현 가능하기 때문입니다.

  2. 학습의 효율성이 좋아집니다. 층을 깊게 하여 학습 데이터 양을 줄여 고속으로 수행할 수 있습니다.

     > 이해가 잘 가지 않는다면 7.6 CNN 시각화하기를 다시 확인하세요.

     - 예를 들어 개를 인식하는 문제를 예시로 들겠습니다.

     - 얕은 신경망은 개의 특징 대부분을 한 번에 이해해야 합니다.

       - 따라서 다양한 견종과 각도 등에 맞추어 변화가 풍부하고 많은 학습 데이터가 필요하며 시간이 오래 걸립니다.

     - 반면에 깊은 신경망은 이 문제를 계층적으로 분해할 수 있습니다.

       - 예를 들어 처음 층은 에지 학습에 전념해 적은 데이터로 효율적으로 학습할 수 있습니다.

  3. 정보를 계층 적으로 전달할 수 있습니다.

     - 예를 들어 에지를 추출한 층의 다음 층은 에지 정보를 사용할 수 있고 더 고도의 패턴을 효과적으로 학습하리라 기대할 수 있습니다.

     - 즉, 층을 깊게 함으로써 각 층이 학습해야 할 문제를 풀기 쉬운 단순한 문제로 분해할 수 있습니다.

- 다만 최근 일어나는 층의 심화는 층이 깊어도 제대로 학습할 수 있는 새로운 기술과 환경(빅데이터와 컴퓨터 연산 능력 등)이 뒷받침되어야 합니다.

## 8.2 딥러닝의 초기 역사

- 딥러닝이 지금처럼 큰 주목을 받게 된 계기는 이미지 인식 기술을 겨루는 장인 ILSVRC 2012 대회이었습니다.

  - 해당 대회에서 딥러닝에 기초한 AlexNet이 압도적인 성적으로 우승하며 이미지 인식에 대한 접근법을 뿌리채 흔들었습니다.

- 이번 장에서는 최근 딥러닝 트렌드를 살펴보겠습니다.

### 8.2.1 이미지넷

- **이미지넷**은 100만 장이 넘는 이미지를 담고 있는 데이터셋입니다.

- 이들은 다음 그림과 같이 다양한 종류의 이미지와 각 이미지이 레이블이 붙어있습니다.

<img src="README.assets/fig 8-7.png" alt="fig 8-7" style="zoom:50%;" />

> 이 이미지 데이터셋을 활용해 자웅을 겨루는 대회가 ILSVRC입니다.

- ILSVRC에는 몇 가지 시험 항목이 있는데 그 중 하나가 **분류**입니다.

  - 분류 부문은 1000개의 클래스를 제대로 분류하는지 겨룹니다.

  - 다음 그래프는 최근까지 ILSVRC 분류 부분 우승팀의 성적으로 **Top-5 오류**를 막대 그래프로 나타내었습니다.

  - 여기서 Top-5 오류란 확률이 가장 높다고 생각하는 후보 클래스 5개 안에 정답이 포함되지 않은, 즉 5개 모두가 틀린 비율입니다.

  <img src="README.assets/fig 8-8.png" alt="fig 8-8" style="zoom:50%;" />

- 여기서 주목할 점은 2012년 이후 선두는 항상 딥러닝을 사용했다는 점입니다.

  - 실제로 2012년 AlexNet이 오류율을 크게 낮췄고 그 후 딥러닝을 활용한 기법이 꾸준히 정확도를 개선했습니다.

  - 특히 2015년 150층이 넘는 심층 신경망인 ResNet이 오류율을 3.5%까지 낮추며 인간의 인식 능력을 넘었다고 평가받고 있습니다.

- 이런 딥러닝 중 최근 몇 년 빼어난 성적을 거두고 있는 VGG, GoogLeNet, ResNet에 대해 간단히 알아보겠습니다.

### 8.2.2 VGG

- VGG는 합성곱 계층과 풀링 계층으로 구성되는 기본적인 CNN입니다.

- 다만, 다음 그림과 같이 비중 있는 층(합성곱 계층, 완전연결 계층)을 모두 16층 혹은 19층으로 심화한게 특징입니다.

<img src="README.assets/fig 8-9.png" alt="fig 8-9" style="zoom:50%;" />

> 층의 깊이에 따라 VGG16 혹은 VGG19로 구분하기도 합니다.

- VGG에서 주목할 점은 (3, 3) 작은 필터를 사용한 합성곱 계층을 연속으로 거친다는 것입니다.

- 합성곱 계층을 2 ~ 4개 연속으로 풀링 계층을 두어 크기를 절반으로 줄이는 처리를 반복하고 마지막에는 완전연결 계층을 통해 결과를 출력합니다.

#### Note

- VGG는 2014년 대회에서 2위를 한 딥러닝입니다.

- 성능 면에서는 GoogLeNet에 뒤지지만 구성이 간단해 응용하기 쉽습니다.

### 8.2.3 GoogLeNet

- GoogLeNet은 다음 그림과 같으며 그림의 사각형이 합성곱 계층과 풀링 계층 등 각 계층을 의미합니다.

<img src="README.assets/fig 8-10.png" alt="fig 8-10" style="zoom:50%;" />

- 구성이 매우 복잡해 보이지만 기본적으로 지금껏 보아온 CNN과 다르지 않습니다.

  - 다만 GoogLeNet은 세로 방향 깊이 말고 가로 방향 깊이도 깊다는 점이 특징입니다.

- GoogLeNet은 가로 반향 폭이 존재하는데 이를 인셉션 구조라고 합니다. 이는 다음과 같습니다.

<img src="README.assets/fig 8-11.png" alt="fig 8-11" style="zoom:50%;" />

- 인셉션 구조는 크기가 다른 필터와 풀링을 여러 개 적용하여 그 결과를 결합합니다.

- 이 구조를 하나의 빌딩 블록, 즉 구성요소로 사용하는 것이 GoogLeNet의 특징입니다.

- 또한 GoogLeNet은 (1, 1) 필터를 사용한 합성곱 계층을 많은 곳에서 사용하여 채널 쪽으로 크기를 줄여 매개변수 제거와 고속 처리에 기여합니다.

### 8.2.4 ResNet

- **ResNet**은 마이크로소프트 팀이 개발한 네트워크로 지금까지보다 층을 더 깊게 할 수 있는 특별한 장치가 존재합니다.

  - 딥러닝의 층을 깊게 하는 건 중요하지만 층이 지나치게 깊으면 학습이 잘 되지 않고 오히려 성능이 떨어지는 경우가 있습니다.

  - 이를 방지하기 위해 ResNet은 **스킵 연결**을 사용합니다.

    - 스킵 연결은 다음 그림처럼 입력 데이터를 합성곱 계층을 건너뛰어 바로 출력에 더하는 구조를 의미합니다.

    <img src="README.assets/fig 8-12.png" alt="fig 8-12" style="zoom:50%;" />

- 이처럼 입력 x를 연속한 두 합성곱 계층을 건너뛰어 출력에 바로 연결하여 출력을 f(x) + x로 만들어줍니다.

- 이 과정을 통해 층이 깊어져도 효율적으로 학습할 수 있도록 해주는데 이는 역전파 때 스킵 연결이 신호 감쇠를 막아주기 때문입니다.

#### Note

- 스킵 연결은 딥력 데이터를 그대로 흘리는 것으로 역전파 때도 상류의 기울기를 그대로 하류로 보냅니다.

- 여기서 핵심은 상류의 기울기가 아무런 수정도 없이 그대로 흐른다는 점입니다.

- 이 덕분에 스킵 연결로 기울기가 작아지거나 지나치게 커질 걱정 없이 앞 층에 의미 있는 기울기가 전해지리라 기대할 수 있습니다.

- 즉, 층을 깊게 할수록 기울기가 작아지는 소실 문제를 스킵 연결이 해결해줍니다.

> 다음 ResNet은 앞서 설명한 VGG 기반에 스킵 연결을 도입한 것입니다.

<img src="README.assets/fig 8-13.png" alt="fig 8-13" style="zoom:50%;" />

- 이와 같이 ResNet은 합성곱 계층을 2개 층마다 건너뛰면서 층을 깊게 합니다.

  - 실험 상 150층 이상으로 해도 정확도가 오르는 모습을 확인할 수 있으며 Top-5 오류율이 3.5%일 정도로 성능이 우수합니다.

#### Note

- 이미지넷이 제공하는 거대한 데이터셋으로 학습한 가중치 값들은 실제 제품에 활용해도 효과적입니다.

- 이를 **전이 학습**이라고 하며 학습된 가중치를 다른 신경망에 복사하여 재학습을 수행합니다.

  - 예를 들어 VGG와 구성이 같은 신경망을 준비하고 미리 학습된 가중치를 초깃값으로 설정한 후 새로운 데이터셋으로 재학습을 수행합니다.

- 이런 전이 학습은 보유한 데이터셋이 적을 때 특히 유용합니다.
