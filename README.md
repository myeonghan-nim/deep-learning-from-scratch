# README

![thumb-course-phthon-basic](README.assets/thumb-course-phthon-basic-1573569963444.jpg)

- Original Github link : [GitHub Repository](https://github.com/WegraLee/deep-learning-from-scratch)

- Deep Learning의 기초부터 차근차근 배워가기 위해 만든 repository 입니다.

#### Tip

- 주로 사용하는 python 라이브러리 설치하기

  - 본 책에서 주로 사용되는 라이브러리는 다음과 같습니다.

  ```bash
  $ pip install numpy scipy matplotlib pandas ipython notebook scikit-learn
  ```

- backups에 대해

  - 해당 폴더는 다음과 같은 파일들을 가지고 있습니다.

    1. `commons.zip`: 각 장에서 공통적으로 사용된 함수, 클래스 등을 정리한 파일입니다.

    2. `datasets.zip`: 각 장에서 공통적으로 사용된 테이터셋과 `pickle` 파일들입니다.

    3. `equations_and_figures.zip`: 각 장에 사용된 수식, 그림들을 압축한 파일입니다.

## What I learned

- [chapter01. Hello, Python](https://github.com/Myeonghan-Jeong/deep-learning-from-scratch/tree/master/chapter01)

  - 1.1 What is python?

  - 1.2 Install python

  - 1.3 Python interpretor

  - 1.4 Python script file

  - 1.5 NumPy

  - 1.6 Matplotlib

  - 1.7 Summary

- [chapter02. Perceptron](https://github.com/Myeonghan-Jeong/deep-learning-from-scratch/tree/master/chapter02)

  - 2.1 What is perceptron?

  - 2.2 Simple logic cycle

  - 2.3 Make perceptron

  - 2.4 Limit of perceptron

  - 2.5 When multilayer perceptron moves

  - 2.6 from NAND to computer

  - 2.7 Summary

- [chapter03. Neural Network](https://github.com/Myeonghan-Jeong/deep-learning-from-scratch/tree/master/chapter03)

  - 3.1 from perceptron to neural network

  - 3.2 Activation function

  - 3.3 Calculation of multi-dimension matrixes

  - 3.4 Simulate multi layer neural network

  - 3.5 Design print layer

  - 3.6 Recognize handwriting

  - 3.7 Summary

- [chapter04. Learning Neural Network](https://github.com/Myeonghan-Jeong/deep-learning-from-scratch/tree/master/chapter04)

  - 4.1 Learn from data!

  - 4.2 Loss function

  - 4.3 Numerical differential

  - 4.4 Slope

  - 4.5 Simulate learning algorithm

  - 4.6 Summary

- [chapter05. Backpropagation](https://github.com/Myeonghan-Jeong/deep-learning-from-scratch/tree/master/chapter05)

  - 5.1 Calculation graph

  - 5.2 Chain rule

  - 5.3 Backpropagation

  - 5.4 Simulate simple layer

  - 5.5 Simulate activation function layer

  - 5.6 Simulate Affine/Softmax layer

  - 5.7 Simulate backpropagation

  - 5.8 Summary

- [chapter06. Technics about learning](https://github.com/Myeonghan-Jeong/deep-learning-from-scratch/tree/master/chapter06)

  - 6.1 Renewal parameters

  - 6.2 Initial value of wieght

  - 6.3 Bacth normalization

  - 6.4 for right learning

  - 6.5 Find suitable hyperparameters

  - 6.6 Summary

- [chapter07. CNN](https://github.com/Myeonghan-Jeong/deep-learning-from-scratch/tree/master/chapter07)

  - 7.1 Overall structure

  - 7.2 Convolutional layer

  - 7.3 Pooling layer

  - 7.4 Simulate convolutional / pooling layer

  - 7.5 Simulate CNN

  - 7.6 Visualize CNN

  - 7.7 Representative CNN

  - 7.8 Summary

- [chapter08. Deep Learning](https://github.com/Myeonghan-Jeong/deep-learning-from-scratch/tree/master/chapter08)

  - 8.1 More deeper

  - 8.2 The early history of deep learning

  - 8.3 More faster(speedy deep learning)

  - 8.4 Application of deep learning

  - 8.5 Future of deep learning

  - 8.6 Summary

- [AppendixA. Softmax-with-Loss Layer`s Calculation Graph](https://github.com/Myeonghan-Jeong/deep-learning-from-scratch/tree/master/chapterAA)

  - A.1 Forward

  - A.2 Backward

  - A.3 Summary
